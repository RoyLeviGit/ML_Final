{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The first thing we will do is import all the libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-all encoding\n",
    "For example: if our target is an integer in the range [0,..,9], so we will have 10 output neuron's in our network.\n",
    "\n",
    "-  If  $y=0$, we want the output neurons to have the values $(1,0,0,0,0,0,0,0,0,0)$\n",
    "\n",
    "-  If  $y=1$ we want the output neurons to have the values $(0,1,0,0,0,0,0,0,0,0)$\n",
    "-  etc\n",
    "\n",
    "The code to covert the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y, out_len):\n",
    "    y_vect = np.zeros((len(y), out_len))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, int(y[i])] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_split_data(data):\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1] - data[:, -1].min()\n",
    "    print(f'Original y hist: {np.histogram(y, range(int(y.max()) + 1))}')\n",
    "    y[y<=2] = 0.\n",
    "    y[y==3] = 1.\n",
    "    y[y>3] = 2.\n",
    "    print(f'Current y hist: {np.histogram(y, [0, 1, 1.9])}')\n",
    "    print(f'y min: {y.min()}, y max: {y.max()}')\n",
    "    y_out_len = int(y.max()) + 1\n",
    "\n",
    "    print(\"The shape of the wines dataset:\")\n",
    "    print(data.shape)\n",
    "    print(y[:10])\n",
    "    print(X[:10, :])\n",
    "\n",
    "    # Scale the dataset\n",
    "    X_scale = StandardScaler()\n",
    "    X = X_scale.fit_transform(X)\n",
    "    print(X) # Looking the new features after scaling\n",
    "\n",
    "    #Split the data into training and test set.  60% training and %40 test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "    # Converting the training and test targets to vectors\n",
    "    y_v_train = convert_y_to_vect(y_train, y_out_len)\n",
    "    y_v_test = convert_y_to_vect(y_test, y_out_len)\n",
    "    # Check to see that our code performs as we expect\n",
    "    print(y_train[0:10])\n",
    "    print(y_v_train[0:10])\n",
    "\n",
    "    return X_train, y_v_train, X_test, y_test, y_out_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative\n",
    "\n",
    "We will use the sigmoid activation function:  $f(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "The deriviative of the sigmoid function is: $f'(z) = f(z)(1-f(z))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W and b\n",
    "We want the weights in W to be different so that during back propagation the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "We want the  weights to be small values, since the sigmoid is almost \"flat\" for large inputs.\n",
    "\n",
    "Next is the code that assigns each weight a number uniformly drawn from $[0.0, 1.0)$.  The code assumes that the number of neurons in each level is in the python list *nn_structure*.\n",
    "\n",
    "In the code, the weights, $W^{(\\ell)}$ and $b^{(\\ell)}$ are held in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n",
    "Creating $\\triangledown W^{(\\ell)}$ and $\\triangledown b^{(\\ell)}$ to have the same size as $W^{(\\ell)}$ and $b^{(\\ell)}$, and setting $\\triangledown W^{(\\ell)}$, and  $\\triangledown b^{(\\ell)}$ to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "Perform a forward pass throught the network.  The function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b, act_func):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = {} # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = act_func(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\delta$\n",
    "The code below compute $\\delta^{(s_l)}$ in a function called \"calculate_out_layer_delta\",  and  computes $\\delta^{(\\ell)}$ for the hidden layers in the function called \"calculate_hidden_delta\".  \n",
    "\n",
    "If we wanted to have a different cost function, we would change the \"calculate_out_layer_delta\" function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out, act_func_deriv):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * act_func_deriv(z_out)\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l, act_func_deriv):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * act_func_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num, act_func, act_func_deriv, l2_lambda=0, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%100 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b, act_func)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l], act_func_deriv)\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l], act_func_deriv)\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * ((1.0/N * tri_W[l]) + (l2_lambda * W[l]))\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers, act_func):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b, act_func)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the avg_cost_func\n",
    "def plot_avg_cost(avg_cost_function, title):\n",
    "    plt.plot(avg_cost_function)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Average J')\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to analyze different methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_performance(title, nn_structure, X_train, y_v_train, X_test, y_test, num_of_iter, act_func, act_func_deriv, l2_lambda=0):\n",
    "    # train the NN\n",
    "    print(f'{title}:')\n",
    "    W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, num_of_iter, act_func, act_func_deriv, l2_lambda)\n",
    "    # Print accuracy of method\n",
    "    y_pred = predict_y(W, b, X_test, 3, act_func)\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print('Prediction accuracy is {}%'.format(accuracy))\n",
    "    # Display plot of average cost per iterations\n",
    "    plot_avg_cost(avg_cost_func, title)\n",
    "\n",
    "    folder = 'nn_logs/'\n",
    "    date = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    np.save(folder+date+title+f'-{accuracy}'+'W', W)\n",
    "    np.save(folder+date+title+f'-{accuracy}'+'b', b)\n",
    "    np.save(folder+date+title+f'-{accuracy}'+'f', avg_cost_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other activation functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [],
   "source": [
    "def re_lu(z):\n",
    "    z_cp = z.copy()\n",
    "    z_cp[z_cp<0] = 0\n",
    "    return z_cp\n",
    "\n",
    "def re_lu_deriv(z):\n",
    "    z_cp = z.copy()\n",
    "    indices_under_zero = z_cp<0\n",
    "    indices_over_zero = z_cp>=0\n",
    "    z_cp[indices_under_zero] = 0\n",
    "    z_cp[indices_over_zero] = 1\n",
    "    return z_cp\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "def tanh_deriv(z):\n",
    "    return 1 - (tanh(z) ** 2)\n",
    "\n",
    "def soft_plus(z):\n",
    "    return np.log(1 + np.exp(z))\n",
    "\n",
    "def soft_plus_deriv(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, y_v_train, X_test, y_test, y_out_len):\n",
    "    print(f'Number of samples: {X_train.shape[0] + X_test.shape[0]}')\n",
    "    nn_structure = [X_train.shape[1], 6, y_out_len] # [11, 6, 3]\n",
    "    num_of_iterations = 3000\n",
    "    l2_lambda = 0.0008\n",
    "\n",
    "    nn_performance(f'Sigmoid, L2={l2_lambda}, Structure={nn_structure}', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, l2_lambda)\n",
    "\n",
    "    nn_structure = [X_train.shape[1], 13, y_out_len] # [11, 13, 3]\n",
    "    num_of_iterations = 3000\n",
    "    l2_lambda = 0.0008\n",
    "\n",
    "    nn_performance(f'Sigmoid, L2={l2_lambda}, Structure={nn_structure}', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, l2_lambda)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [],
   "source": [
    "df_red = pd.read_csv('winequality-red.csv', sep=';')\n",
    "data_red = df_red.to_numpy()\n",
    "\n",
    "df_white = pd.read_csv('winequality-white.csv', sep=';')\n",
    "data_white = df_white.to_numpy()\n",
    "\n",
    "data_all = np.vstack((data_red, data_white))\n",
    "\n",
    "rs = r.RandomState(42)\n",
    "r.shuffle(data_red)\n",
    "r.shuffle(data_white)\n",
    "r.shuffle(data_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Red Wines Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y hist: (array([ 10,  53, 681, 638, 217]), array([0, 1, 2, 3, 4, 5]))\n",
      "Current y hist: (array([744, 638]), array([0. , 1. , 1.9]))\n",
      "y min: 0.0, y max: 2.0\n",
      "The shape of the wines dataset:\n",
      "(1599, 12)\n",
      "[2. 0. 0. 1. 1. 2. 2. 0. 2. 2.]\n",
      "[[5.4000e+00 4.2000e-01 2.7000e-01 2.0000e+00 9.2000e-02 2.3000e+01\n",
      "  5.5000e+01 9.9471e-01 3.7800e+00 6.4000e-01 1.2300e+01]\n",
      " [6.6000e+00 6.3000e-01 0.0000e+00 4.3000e+00 9.3000e-02 5.1000e+01\n",
      "  7.7500e+01 9.9558e-01 3.2000e+00 4.5000e-01 9.5000e+00]\n",
      " [8.2000e+00 3.9000e-01 4.9000e-01 2.3000e+00 9.9000e-02 4.7000e+01\n",
      "  1.3300e+02 9.9790e-01 3.3800e+00 9.9000e-01 9.8000e+00]\n",
      " [8.1000e+00 8.2500e-01 2.4000e-01 2.1000e+00 8.4000e-02 5.0000e+00\n",
      "  1.3000e+01 9.9720e-01 3.3700e+00 7.7000e-01 1.0700e+01]\n",
      " [8.2000e+00 3.8000e-01 3.2000e-01 2.5000e+00 8.0000e-02 2.4000e+01\n",
      "  7.1000e+01 9.9624e-01 3.2700e+00 8.5000e-01 1.1000e+01]\n",
      " [7.8000e+00 5.7000e-01 9.0000e-02 2.3000e+00 6.5000e-02 3.4000e+01\n",
      "  4.5000e+01 9.9417e-01 3.4600e+00 7.4000e-01 1.2700e+01]\n",
      " [8.2000e+00 3.3000e-01 3.9000e-01 2.5000e+00 7.4000e-02 2.9000e+01\n",
      "  4.8000e+01 9.9528e-01 3.3200e+00 8.8000e-01 1.2400e+01]\n",
      " [7.7000e+00 6.9000e-01 2.2000e-01 1.9000e+00 8.4000e-02 1.8000e+01\n",
      "  9.4000e+01 9.9610e-01 3.3100e+00 4.8000e-01 9.5000e+00]\n",
      " [1.1100e+01 3.1000e-01 5.3000e-01 2.2000e+00 6.0000e-02 3.0000e+00\n",
      "  1.0000e+01 9.9572e-01 3.0200e+00 8.3000e-01 1.0900e+01]\n",
      " [6.6000e+00 8.4000e-01 3.0000e-02 2.3000e+00 5.9000e-02 3.2000e+01\n",
      "  4.8000e+01 9.9520e-01 3.5200e+00 5.6000e-01 1.2300e+01]]\n",
      "[[-1.6774205  -0.60233677 -0.0050098  ...  3.03804801 -0.10710191\n",
      "   1.76190411]\n",
      " [-0.98798397  0.57082331 -1.39147228 ... -0.7199333  -1.22835037\n",
      "  -0.86637886]\n",
      " [-0.06873526 -0.76993107  1.12470036 ...  0.44633676  1.95835578\n",
      "  -0.58477711]\n",
      " ...\n",
      " [ 0.96541954 -0.65820153  0.91929852 ... -0.65514052 -0.99229806\n",
      "   1.29256787]\n",
      " [-1.04543701  0.79428237 -1.39147228 ...  0.89988623 -0.57920652\n",
      "   1.29256787]\n",
      " [-0.70071875  0.73841761 -1.39147228 ...  0.96467901 -0.69723268\n",
      "  -0.86637886]]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_v_train, X_test, y_test, y_out_len = transform_and_split_data(data_red)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1599\n",
      "Sigmoid, L2=0.0008, Structure=[11, 6, 3]:\n",
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 100 of 3000\n",
      "Iteration 200 of 3000\n",
      "Iteration 300 of 3000\n",
      "Iteration 400 of 3000\n",
      "Iteration 500 of 3000\n",
      "Iteration 600 of 3000\n",
      "Iteration 700 of 3000\n",
      "Iteration 800 of 3000\n",
      "Iteration 900 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 1100 of 3000\n",
      "Iteration 1200 of 3000\n",
      "Iteration 1300 of 3000\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(X_train, y_v_train, X_test, y_test, y_out_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train White Wines Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_v_train, X_test, y_test, y_out_len = transform_and_split_data(data_white)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_neural_network(X_train, y_v_train, X_test, y_test, y_out_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train All Wines Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_v_train, X_test, y_test, y_out_len = transform_and_split_data(data_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_neural_network(X_train, y_v_train, X_test, y_test, y_out_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run performance analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nn_performance('Sigmoid Activation Function', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv)\n",
    "# Section a\n",
    "# nn_performance('Sigmoid, L2 Regularization (lambda=0.01)', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, 0.01)\n",
    "# Section b\n",
    "# nn_performance('ReLU Activation Function',nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, re_lu, re_lu_deriv)\n",
    "# Section c\n",
    "# nn_performance('Tanh Activation Function', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, tanh, tanh_deriv)\n",
    "# Section d\n",
    "# nn_performance('Soft-plus Activation Function', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, soft_plus, soft_plus_deriv)\n",
    "# Section e\n",
    "# nn_performance('Sigmoid, 4000 Iterations (Not 3000)', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations + 1000, f, f_deriv)\n",
    "# Section f\n",
    "# diff_nn_structure = [X.shape[1], 16, y_out_len]\n",
    "# nn_performance('Sigmoid, NN Structure = [11, 45 (Not 30), 10]', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv)\n",
    "\n",
    "# for i in list(range(6, 7)) + list(range(11, 14)):\n",
    "#     nn_structure[1] = i\n",
    "#     print(f'Using NN Structure: {nn_structure}')\n",
    "#     nn_performance('Sigmoid, L2 Regularization (lambda=0.001)', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, 0.001)\n",
    "\n",
    "# Checking different l2 lambdas\n",
    "# for l2_lambda in [0.0008]:\n",
    "#     nn_performance(f'Sigmoid, L2={l2_lambda}, Structure={nn_structure}', nn_structure, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, l2_lambda)\n",
    "\n",
    "# Checking different NN structures\n",
    "# for nn in [nn_structure, [X.shape[1], 13, 11, y_out_len], [X.shape[1], 13, 6, y_out_len], [X.shape[1], 13, 3, y_out_len]]: # [11, 13, 3]]\n",
    "#     nn_performance(f'Sigmoid, L2={l2_lambda}, Structure={nn}', nn, X_train, y_v_train, X_test, y_test, num_of_iterations, f, f_deriv, l2_lambda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}